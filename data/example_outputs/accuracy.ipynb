{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def levenshtein_distance(s1, s2):\n",
    "    \"\"\"\n",
    "    Returns the Levenshtein edit distance between two strings s1 and s2.\n",
    "    The distance is the minimum number of single-character edits\n",
    "    (insertions, deletions, substitutions) required to transform s1 into s2.\n",
    "    \"\"\"\n",
    "    if not s1:\n",
    "        return len(s2)\n",
    "    if not s2:\n",
    "        return len(s1)\n",
    "\n",
    "    rows = len(s1) + 1\n",
    "    cols = len(s2) + 1\n",
    "    dp = [[0]*cols for _ in range(rows)]\n",
    "\n",
    "    for i in range(rows):\n",
    "        dp[i][0] = i\n",
    "    for j in range(cols):\n",
    "        dp[0][j] = j\n",
    "\n",
    "    for i in range(1, rows):\n",
    "        for j in range(1, cols):\n",
    "            cost = 0 if s1[i-1] == s2[j-1] else 1\n",
    "            dp[i][j] = min(\n",
    "                dp[i-1][j] + 1,\n",
    "                dp[i][j-1] + 1,\n",
    "                dp[i-1][j-1] + cost\n",
    "            )\n",
    "    return dp[-1][-1]\n",
    "\n",
    "\n",
    "def digit_level_accuracy(true_val, pred_val):\n",
    "    \"\"\"\n",
    "    Compares two numeric values (or strings) digit by digit,\n",
    "    counting how many match exactly. Returns (matched_digits, total_digits_in_truth).\n",
    "    \"\"\"\n",
    "    t_str = str(true_val)\n",
    "    p_str = str(pred_val)\n",
    "\n",
    "    matched = 0\n",
    "    total = len(t_str)\n",
    "\n",
    "\n",
    "    min_len = min(len(t_str), len(p_str))\n",
    "    for i in range(min_len):\n",
    "        if t_str[i] == p_str[i]:\n",
    "            matched += 1\n",
    "\n",
    "    return matched, total\n",
    "\n",
    "\n",
    "def evaluate_accuracy(df_true, df_pred, numeric_cols, text_cols):\n",
    "    \"\"\"\n",
    "    Given two DataFrames (human-verified truth, AI-extracted) with the same shape/rows,\n",
    "    computes:\n",
    "      - Overall digit-level accuracy for numeric columns\n",
    "      - Average Levenshtein distance for text columns\n",
    "    Returns a dict with metrics.\n",
    "    \"\"\"\n",
    "\n",
    "    total_matched_digits = 0\n",
    "    total_groundtruth_digits = 0\n",
    "    for col in numeric_cols:\n",
    "        for i in range(len(df_true)):\n",
    "            matched, total = digit_level_accuracy(df_true[col].iloc[i], df_pred[col].iloc[i])\n",
    "            total_matched_digits += matched\n",
    "            total_groundtruth_digits += total\n",
    "\n",
    "    numeric_accuracy = 0.0\n",
    "    if total_groundtruth_digits > 0:\n",
    "        numeric_accuracy = total_matched_digits / total_groundtruth_digits\n",
    "\n",
    "    total_lev_dist = 0\n",
    "    count_entries = 0\n",
    "    for col in text_cols:\n",
    "        for i in range(len(df_true)):\n",
    "            true_str = str(df_true[col].iloc[i])\n",
    "            pred_str = str(df_pred[col].iloc[i])\n",
    "            dist = levenshtein_distance(true_str, pred_str)\n",
    "            total_lev_dist += dist\n",
    "            count_entries += 1\n",
    "\n",
    "    avg_lev_dist = 0.0\n",
    "    if count_entries > 0:\n",
    "        avg_lev_dist = total_lev_dist / count_entries\n",
    "\n",
    "    return {\n",
    "        \"digit_level_accuracy\": numeric_accuracy,\n",
    "        \"avg_levenshtein_dist\": avg_lev_dist\n",
    "    }\n",
    "\n",
    "def compare_digits_with_confusion(true_val, pred_val, confusion_matrix):\n",
    "    \"\"\"\n",
    "    Compares two numeric values (or strings) digit by digit.\n",
    "    Updates a 10x10 'confusion_matrix' with counts of (true_digit, pred_digit).\n",
    "\n",
    "    Returns:\n",
    "      matched_digits: int\n",
    "      total_true_digits: int\n",
    "    \"\"\"\n",
    "    t_str = str(true_val)\n",
    "    p_str = str(pred_val)\n",
    "\n",
    "    matched = 0\n",
    "    total = len(t_str)\n",
    "\n",
    "    min_len = min(len(t_str), len(p_str))\n",
    "    for i in range(min_len):\n",
    "        gt_digit = t_str[i]\n",
    "        pred_digit = p_str[i]\n",
    "\n",
    "        if gt_digit.isdigit() and pred_digit.isdigit():\n",
    "            gt_idx = int(gt_digit)\n",
    "            pd_idx = int(pred_digit)\n",
    "            confusion_matrix[gt_idx, pd_idx] += 1\n",
    "\n",
    "            if gt_digit == pred_digit:\n",
    "                matched += 1\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    return matched, total\n",
    "\n",
    "# Cell 3: Evaluate numeric accuracy with confusion details\n",
    "\n",
    "def evaluate_numeric_with_confusion(df_true, df_pred, numeric_cols):\n",
    "    import numpy as np\n",
    "\n",
    "    master_confusion = np.zeros((10,10), dtype=int)\n",
    "\n",
    "    # Track overall matched/total\n",
    "    overall_matched = 0\n",
    "    overall_total   = 0\n",
    "\n",
    "    # Per-column results\n",
    "    per_column_stats = {}\n",
    "\n",
    "    for col in numeric_cols:\n",
    "        col_conf = np.zeros((10,10), dtype=int)\n",
    "        col_matched = 0\n",
    "        col_total = 0\n",
    "\n",
    "        for i in range(len(df_true)):\n",
    "            true_val = df_true[col].iloc[i]\n",
    "            pred_val = df_pred[col].iloc[i]\n",
    "\n",
    "            # Compare digits, update *column* confusion\n",
    "            m, t = compare_digits_with_confusion(true_val, pred_val, col_conf)\n",
    "            # Also update the *master* confusion for all numeric columns\n",
    "            compare_digits_with_confusion(true_val, pred_val, master_confusion)\n",
    "\n",
    "            col_matched += m\n",
    "            col_total   += t\n",
    "\n",
    "        # per-column digit-level accuracy\n",
    "        col_acc = col_matched / col_total if col_total > 0 else 0\n",
    "\n",
    "        per_column_stats[col] = {\n",
    "            \"digit_level_accuracy\": col_acc,\n",
    "            \"confusion_matrix\": col_conf\n",
    "        }\n",
    "\n",
    "        overall_matched += col_matched\n",
    "        overall_total   += col_total\n",
    "\n",
    "    # Overall digit-level accuracy across all numeric columns\n",
    "    overall_acc = overall_matched / overall_total if overall_total > 0 else 0\n",
    "\n",
    "    # Return the top-level fields matching the usage code\n",
    "    return {\n",
    "        \"digit_level_accuracy\": overall_acc,       # formerly \"overall_digit_accuracy\"\n",
    "        \"confusion_matrix\": master_confusion,      # formerly \"master_confusion\"\n",
    "        \"per_column\": per_column_stats\n",
    "    }\n",
    "\n",
    "\n",
    "def analyze_digit_confusion(confusion_matrix):\n",
    "    \"\"\"\n",
    "    Prints info about digit confusion: top confusions, etc.\n",
    "    \"\"\"\n",
    "    print(\"Digit Confusion Matrix (rows=ground-truth, cols=prediction):\")\n",
    "    print(confusion_matrix)\n",
    "    print()\n",
    "\n",
    "    row_sums = confusion_matrix.sum(axis=1)\n",
    "    for digit in range(10):\n",
    "        total_count = row_sums[digit]\n",
    "        if total_count == 0:\n",
    "            continue\n",
    "        row = confusion_matrix[digit]\n",
    "        sorted_preds = sorted(\n",
    "            range(10),\n",
    "            key=lambda x: row[x],\n",
    "            reverse=True\n",
    "        )\n",
    "\n",
    "        top_pred = sorted_preds[0]\n",
    "        top_count = row[top_pred]\n",
    "\n",
    "        if len(sorted_preds) > 1:\n",
    "            second_pred = sorted_preds[1]\n",
    "            second_count = row[second_pred]\n",
    "        else:\n",
    "            second_pred = None\n",
    "            second_count = 0\n",
    "\n",
    "        print(f\"Ground Truth Digit: {digit}\")\n",
    "        print(f\"  Most predicted as: {top_pred} ({top_count} times)\")\n",
    "        if digit != top_pred:\n",
    "            print(f\"  --> This indicates confusion of {digit} -> {top_pred}\")\n",
    "        # Show second\n",
    "        if second_pred is not None:\n",
    "            print(f\"  Second predicted as: {second_pred} ({second_count} times)\")\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_chars_with_confusion(true_str, pred_str, confusion_matrix, char_list):\n",
    "    \"\"\"\n",
    "    Compare two strings character-by-character, updating confusion_matrix\n",
    "    (size = len(char_list) x len(char_list)).\n",
    "\n",
    "    - `char_list` is a list or string of valid characters. For example,\n",
    "      you might use `string.ascii_lowercase` for 'a'..'z'.\n",
    "    - We convert both strings to lowercase here (you can do more normalization if desired).\n",
    "\n",
    "    We do not handle insertions/deletions specially. We only compare up to min_len.\n",
    "    \"\"\"\n",
    "    t_str = true_str.lower()\n",
    "    p_str = pred_str.lower()\n",
    "\n",
    "    min_len = min(len(t_str), len(p_str))\n",
    "    for i in range(min_len):\n",
    "        gt_char = t_str[i]\n",
    "        pd_char = p_str[i]\n",
    "\n",
    "        if gt_char in char_list and pd_char in char_list:\n",
    "            gt_idx = char_list.index(gt_char)\n",
    "            pd_idx = char_list.index(pd_char)\n",
    "            confusion_matrix[gt_idx, pd_idx] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_text_with_char_confusion(df_true, df_pred, text_cols, char_list):\n",
    "    \"\"\"\n",
    "    For each text column, build a character-level confusion matrix.\n",
    "    Also track exact string match rate per column.\n",
    "\n",
    "    Returns a dictionary with:\n",
    "      - \"per_column\": { col_name: { \"char_confusion_matrix\": ..., \"exact_match_rate\": ...}, ... }\n",
    "      - \"master_char_confusion\": big confusion matrix aggregated across all text cols\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "\n",
    "    size = len(char_list)\n",
    "    master_confusion = np.zeros((size, size), dtype=int)\n",
    "\n",
    "    per_col_results = {}\n",
    "\n",
    "    for col in text_cols:\n",
    "        col_confusion = np.zeros((size, size), dtype=int)\n",
    "\n",
    "        exact_matches = 0\n",
    "        total_rows = len(df_true)\n",
    "\n",
    "        for i in range(total_rows):\n",
    "            true_str = str(df_true[col].iloc[i])\n",
    "            pred_str = str(df_pred[col].iloc[i])\n",
    "\n",
    "            compare_chars_with_confusion(true_str, pred_str, col_confusion, char_list)\n",
    "            compare_chars_with_confusion(true_str, pred_str, master_confusion, char_list)\n",
    "\n",
    "            if true_str.strip().lower() == pred_str.strip().lower():\n",
    "                exact_matches += 1\n",
    "\n",
    "        exact_rate = exact_matches / total_rows if total_rows > 0 else 0\n",
    "\n",
    "        per_col_results[col] = {\n",
    "            \"char_confusion_matrix\": col_confusion,\n",
    "            \"exact_match_rate\": exact_rate\n",
    "        }\n",
    "\n",
    "    return {\n",
    "        \"per_column\": per_col_results,\n",
    "        \"master_char_confusion\": master_confusion\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_char_confusion(confusion_matrix, char_list):\n",
    "    \"\"\"\n",
    "    Prints or interprets a character confusion matrix (rows=GT char, cols=Pred char).\n",
    "    Now includes the second most commonly predicted character for each row.\n",
    "    \"\"\"\n",
    "    row_sums = confusion_matrix.sum(axis=1)\n",
    "\n",
    "    print(\"Character Confusion Matrix (rows=GT, cols=Prediction). Shape:\", confusion_matrix.shape)\n",
    "    print()\n",
    "\n",
    "    for i, row_total in enumerate(row_sums):\n",
    "        if row_total == 0:\n",
    "            continue\n",
    "\n",
    "        gt_char = char_list[i]\n",
    "        row = confusion_matrix[i]\n",
    "\n",
    "        sorted_preds = sorted(range(len(char_list)), key=lambda x: row[x], reverse=True)\n",
    "\n",
    "        top_pred_idx = sorted_preds[0]\n",
    "        top_count = row[top_pred_idx]\n",
    "\n",
    "        if len(sorted_preds) > 1:\n",
    "            second_pred_idx = sorted_preds[1]\n",
    "            second_count = row[second_pred_idx]\n",
    "        else:\n",
    "            second_pred_idx = None\n",
    "            second_count = 0\n",
    "\n",
    "        print(f\"Ground Truth Character: '{gt_char}' (count in GT = {row_total})\")\n",
    "\n",
    "        top_pred_char = char_list[top_pred_idx]\n",
    "        print(f\"  Most commonly predicted as: '{top_pred_char}' (count={top_count})\")\n",
    "\n",
    "        if top_pred_char == gt_char:\n",
    "            if second_pred_idx is not None:\n",
    "                second_pred_char = char_list[second_pred_idx]\n",
    "                print(f\"  Second most predicted (possible confusion): '{second_pred_char}' (count={second_count})\")\n",
    "        else:\n",
    "            print(f\"    --> Confusion: '{gt_char}' -> '{top_pred_char}'\")\n",
    "            if second_pred_idx is not None:\n",
    "                second_pred_char = char_list[second_pred_idx]\n",
    "                print(f\"  Second predicted: '{second_pred_char}' (count={second_count})\")\n",
    "\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_numeric_errors(df_true, df_pred, numeric_cols, debug=False):\n",
    "    \"\"\"\n",
    "    Computes Mean Absolute Error (MAE) and Mean Absolute Percentage Error (MAPE)\n",
    "    for the specified numeric_cols, both overall and per-column.\n",
    "\n",
    "    If debug=True, prints out row-by-row details about parsing and detection of NaN/inf.\n",
    "\n",
    "    Returns a dict:\n",
    "    {\n",
    "      \"overall_mae\": float,\n",
    "      \"overall_mape\": float,\n",
    "      \"per_column\": {\n",
    "          col_name: {\n",
    "              \"mae\": float,\n",
    "              \"mape\": float\n",
    "          }, ...\n",
    "      }\n",
    "    }\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "\n",
    "    overall_abs_errors = []\n",
    "    overall_pct_errors = []\n",
    "\n",
    "    per_column_stats = {}\n",
    "\n",
    "    # Track how many rows were successfully parsed (for debugging)\n",
    "    parse_success_count = {col: 0 for col in numeric_cols}\n",
    "    parse_fail_count    = {col: 0 for col in numeric_cols}\n",
    "\n",
    "    # Also track how many rows we skip due to NaN or inf\n",
    "    nan_or_inf_count = {col: 0 for col in numeric_cols}\n",
    "\n",
    "    for col in numeric_cols:\n",
    "        col_abs_errors = []\n",
    "        col_pct_errors = []\n",
    "\n",
    "        for i in range(len(df_true)):\n",
    "            true_val = df_true[col].iloc[i]\n",
    "            pred_val = df_pred[col].iloc[i]\n",
    "\n",
    "            # 1) Attempt to parse ground-truth as float\n",
    "            try:\n",
    "                true_num = float(str(true_val).replace(\",\", \"\").strip())\n",
    "            except:\n",
    "                if debug:\n",
    "                    print(f\"[DEBUG] Row {i}, col '{col}' -> Failed to parse true_val = {true_val!r}\")\n",
    "                parse_fail_count[col] += 1\n",
    "                continue\n",
    "\n",
    "            # 2) Attempt to parse predicted as float\n",
    "            try:\n",
    "                pred_num = float(str(pred_val).replace(\",\", \"\").strip())\n",
    "            except:\n",
    "                if debug:\n",
    "                    print(f\"[DEBUG] Row {i}, col '{col}' -> Failed to parse pred_val = {pred_val!r}\")\n",
    "                parse_fail_count[col] += 1\n",
    "                continue\n",
    "\n",
    "            # 3) Check if either is nan or inf\n",
    "            if (np.isnan(true_num) or np.isnan(pred_num)\n",
    "                or np.isinf(true_num) or np.isinf(pred_num)):\n",
    "                if debug:\n",
    "                    print(f\"[DEBUG] Row {i}, col '{col}' -> true_num={true_num}, pred_num={pred_num}, skipping (NaN/inf).\")\n",
    "                nan_or_inf_count[col] += 1\n",
    "                continue\n",
    "\n",
    "            # If we reach here, we have valid finite floats\n",
    "            parse_success_count[col] += 1\n",
    "\n",
    "            # 4) Calculate absolute error\n",
    "            abs_err = abs(pred_num - true_num)\n",
    "            col_abs_errors.append(abs_err)\n",
    "            overall_abs_errors.append(abs_err)\n",
    "\n",
    "            # 5) Calculate % error if true_num != 0\n",
    "            if true_num != 0:\n",
    "                pct_err = abs_err / abs(true_num)\n",
    "                col_pct_errors.append(pct_err)\n",
    "                overall_pct_errors.append(pct_err)\n",
    "            else:\n",
    "                # If you want to treat \"true == 0 but pred != 0\" as 100% error,\n",
    "                # you could do: col_pct_errors.append(1.0) etc.\n",
    "                # For now, we skip it.\n",
    "                pass\n",
    "\n",
    "        # 6) Compute MAE and MAPE for this column\n",
    "        if len(col_abs_errors) > 0:\n",
    "            col_mae  = float(np.mean(col_abs_errors))\n",
    "        else:\n",
    "            col_mae  = float('nan')\n",
    "\n",
    "        if len(col_pct_errors) > 0:\n",
    "            col_mape = float(np.mean(col_pct_errors))\n",
    "        else:\n",
    "            col_mape = float('nan')\n",
    "\n",
    "        per_column_stats[col] = {\n",
    "            \"mae\":  col_mae,\n",
    "            \"mape\": col_mape\n",
    "        }\n",
    "\n",
    "    # 7) Overall errors\n",
    "    if len(overall_abs_errors) > 0:\n",
    "        overall_mae = float(np.mean(overall_abs_errors))\n",
    "    else:\n",
    "        overall_mae = float('nan')\n",
    "\n",
    "    if len(overall_pct_errors) > 0:\n",
    "        overall_mape = float(np.mean(overall_pct_errors))\n",
    "    else:\n",
    "        overall_mape = float('nan')\n",
    "\n",
    "    # 8) Debug summary\n",
    "    if debug:\n",
    "        print(\"\\n[DEBUG] Parsing & Skipping Summary for Numeric Columns:\")\n",
    "        for col in numeric_cols:\n",
    "            print(f\"  {col}: parsed={parse_success_count[col]}, parse_fail={parse_fail_count[col]}, nan_or_inf_skips={nan_or_inf_count[col]}\")\n",
    "        print()\n",
    "\n",
    "    return {\n",
    "        \"overall_mae\":  overall_mae,\n",
    "        \"overall_mape\": overall_mape,\n",
    "        \"per_column\":   per_column_stats\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 1920 Page 9 ===\n",
      "Overall Digit-Level Accuracy: 0.9850427350427351\n",
      "Average Levenshtein Distance (text): 0.03353658536585366\n",
      "---------------------------------\n",
      "Digit-Level Accuracy (via confusion approach): 0.8675213675213675\n",
      "Digit Confusion Matrix (rows=ground-truth, cols=prediction):\n",
      "[[178   0   0   0   0   0   0   0   0   1]\n",
      " [  0 132   0   0   0   0   0   0   0   0]\n",
      " [  0   0 132   0   0   0   0   0   0   0]\n",
      " [  0   0   0  44   2   0   0   0   0   0]\n",
      " [  0   0   1   0  44   0   0   0   0   0]\n",
      " [  0   0   0   1   0  44   0   0   2   1]\n",
      " [  0   1   0   0   0   0  36   0   1   0]\n",
      " [  0   0   0   0   0   0   0  46   0   0]\n",
      " [  0   0   0   1   0   0   0   0  36   0]\n",
      " [  2   0   0   0   1   0   0   0   0 120]]\n",
      "\n",
      "Ground Truth Digit: 0\n",
      "  Most predicted as: 0 (178 times)\n",
      "  Second predicted as: 9 (1 times)\n",
      "\n",
      "Ground Truth Digit: 1\n",
      "  Most predicted as: 1 (132 times)\n",
      "  Second predicted as: 0 (0 times)\n",
      "\n",
      "Ground Truth Digit: 2\n",
      "  Most predicted as: 2 (132 times)\n",
      "  Second predicted as: 0 (0 times)\n",
      "\n",
      "Ground Truth Digit: 3\n",
      "  Most predicted as: 3 (44 times)\n",
      "  Second predicted as: 4 (2 times)\n",
      "\n",
      "Ground Truth Digit: 4\n",
      "  Most predicted as: 4 (44 times)\n",
      "  Second predicted as: 2 (1 times)\n",
      "\n",
      "Ground Truth Digit: 5\n",
      "  Most predicted as: 5 (44 times)\n",
      "  Second predicted as: 8 (2 times)\n",
      "\n",
      "Ground Truth Digit: 6\n",
      "  Most predicted as: 6 (36 times)\n",
      "  Second predicted as: 1 (1 times)\n",
      "\n",
      "Ground Truth Digit: 7\n",
      "  Most predicted as: 7 (46 times)\n",
      "  Second predicted as: 0 (0 times)\n",
      "\n",
      "Ground Truth Digit: 8\n",
      "  Most predicted as: 8 (36 times)\n",
      "  Second predicted as: 3 (1 times)\n",
      "\n",
      "Ground Truth Digit: 9\n",
      "  Most predicted as: 9 (120 times)\n",
      "  Second predicted as: 0 (2 times)\n",
      "\n",
      "---------------------------------\n",
      "Text Character Confusion:\n",
      "Character Confusion Matrix (rows=GT, cols=Prediction). Shape: (26, 26)\n",
      "\n",
      "Ground Truth Character: 'a' (count in GT = 192)\n",
      "  Most commonly predicted as: 'a' (count=192)\n",
      "  Second most predicted (possible confusion): 'b' (count=0)\n",
      "\n",
      "Ground Truth Character: 'b' (count in GT = 47)\n",
      "  Most commonly predicted as: 'b' (count=47)\n",
      "  Second most predicted (possible confusion): 'a' (count=0)\n",
      "\n",
      "Ground Truth Character: 'c' (count in GT = 136)\n",
      "  Most commonly predicted as: 'c' (count=136)\n",
      "  Second most predicted (possible confusion): 'a' (count=0)\n",
      "\n",
      "Ground Truth Character: 'd' (count in GT = 72)\n",
      "  Most commonly predicted as: 'd' (count=71)\n",
      "  Second most predicted (possible confusion): 's' (count=1)\n",
      "\n",
      "Ground Truth Character: 'e' (count in GT = 274)\n",
      "  Most commonly predicted as: 'e' (count=271)\n",
      "  Second most predicted (possible confusion): 'f' (count=1)\n",
      "\n",
      "Ground Truth Character: 'f' (count in GT = 16)\n",
      "  Most commonly predicted as: 'f' (count=15)\n",
      "  Second most predicted (possible confusion): 'd' (count=1)\n",
      "\n",
      "Ground Truth Character: 'g' (count in GT = 17)\n",
      "  Most commonly predicted as: 'g' (count=17)\n",
      "  Second most predicted (possible confusion): 'a' (count=0)\n",
      "\n",
      "Ground Truth Character: 'h' (count in GT = 100)\n",
      "  Most commonly predicted as: 'h' (count=98)\n",
      "  Second most predicted (possible confusion): 'j' (count=1)\n",
      "\n",
      "Ground Truth Character: 'i' (count in GT = 324)\n",
      "  Most commonly predicted as: 'i' (count=323)\n",
      "  Second most predicted (possible confusion): 'm' (count=1)\n",
      "\n",
      "Ground Truth Character: 'j' (count in GT = 18)\n",
      "  Most commonly predicted as: 'j' (count=18)\n",
      "  Second most predicted (possible confusion): 'a' (count=0)\n",
      "\n",
      "Ground Truth Character: 'k' (count in GT = 14)\n",
      "  Most commonly predicted as: 'k' (count=14)\n",
      "  Second most predicted (possible confusion): 'a' (count=0)\n",
      "\n",
      "Ground Truth Character: 'l' (count in GT = 123)\n",
      "  Most commonly predicted as: 'l' (count=122)\n",
      "  Second most predicted (possible confusion): 'i' (count=1)\n",
      "\n",
      "Ground Truth Character: 'm' (count in GT = 143)\n",
      "  Most commonly predicted as: 'm' (count=143)\n",
      "  Second most predicted (possible confusion): 'a' (count=0)\n",
      "\n",
      "Ground Truth Character: 'n' (count in GT = 194)\n",
      "  Most commonly predicted as: 'n' (count=192)\n",
      "  Second most predicted (possible confusion): 'a' (count=1)\n",
      "\n",
      "Ground Truth Character: 'o' (count in GT = 237)\n",
      "  Most commonly predicted as: 'o' (count=234)\n",
      "  Second most predicted (possible confusion): 'h' (count=1)\n",
      "\n",
      "Ground Truth Character: 'p' (count in GT = 90)\n",
      "  Most commonly predicted as: 'p' (count=90)\n",
      "  Second most predicted (possible confusion): 'a' (count=0)\n",
      "\n",
      "Ground Truth Character: 'q' (count in GT = 1)\n",
      "  Most commonly predicted as: 'q' (count=1)\n",
      "  Second most predicted (possible confusion): 'a' (count=0)\n",
      "\n",
      "Ground Truth Character: 'r' (count in GT = 172)\n",
      "  Most commonly predicted as: 'r' (count=170)\n",
      "  Second most predicted (possible confusion): 'e' (count=1)\n",
      "\n",
      "Ground Truth Character: 's' (count in GT = 363)\n",
      "  Most commonly predicted as: 's' (count=362)\n",
      "  Second most predicted (possible confusion): 'o' (count=1)\n",
      "\n",
      "Ground Truth Character: 't' (count in GT = 128)\n",
      "  Most commonly predicted as: 't' (count=128)\n",
      "  Second most predicted (possible confusion): 'a' (count=0)\n",
      "\n",
      "Ground Truth Character: 'u' (count in GT = 143)\n",
      "  Most commonly predicted as: 'u' (count=143)\n",
      "  Second most predicted (possible confusion): 'a' (count=0)\n",
      "\n",
      "Ground Truth Character: 'v' (count in GT = 5)\n",
      "  Most commonly predicted as: 'v' (count=5)\n",
      "  Second most predicted (possible confusion): 'a' (count=0)\n",
      "\n",
      "Ground Truth Character: 'w' (count in GT = 20)\n",
      "  Most commonly predicted as: 'w' (count=20)\n",
      "  Second most predicted (possible confusion): 'a' (count=0)\n",
      "\n",
      "Ground Truth Character: 'y' (count in GT = 12)\n",
      "  Most commonly predicted as: 'y' (count=11)\n",
      "  Second most predicted (possible confusion): 'e' (count=1)\n",
      "\n",
      "Ground Truth Character: 'z' (count in GT = 1)\n",
      "  Most commonly predicted as: 'z' (count=1)\n",
      "  Second most predicted (possible confusion): 'a' (count=0)\n",
      "\n",
      "---------------------------------\n",
      "Column: STATE\n",
      "  Exact Match Rate: 1.00\n",
      "Column: RACE_TYPE\n",
      "  Exact Match Rate: 1.00\n",
      "Column: CANDIDATE_NAME\n",
      "  Exact Match Rate: 0.93\n",
      "Column: CANDIDATE_PARTY\n",
      "  Exact Match Rate: 1.00\n",
      "===============================================\n",
      "\n",
      "\n",
      "=== 1940 Page 1 ===\n",
      "Overall Digit-Level Accuracy: 0.9908814589665653\n",
      "Average Levenshtein Distance (text): 0.7241379310344828\n",
      "---------------------------------\n",
      "Digit-Level Accuracy (via confusion approach): 0.8115501519756839\n",
      "Digit Confusion Matrix (rows=ground-truth, cols=prediction):\n",
      "[[58  0  0  0  0  0  0  0  0  0]\n",
      " [ 0 45  0  0  0  0  0  0  0  0]\n",
      " [ 0  0 20  0  0  0  0  0  0  0]\n",
      " [ 0  0  0 19  0  0  0  0  0  0]\n",
      " [ 0  0  0  0 41  0  0  0  0  0]\n",
      " [ 0  0  0  0  0 10  0  0  0  0]\n",
      " [ 3  0  0  0  0  0 10  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 10  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  9  0]\n",
      " [ 0  0  0  0  0  0  0  0  0 45]]\n",
      "\n",
      "Ground Truth Digit: 0\n",
      "  Most predicted as: 0 (58 times)\n",
      "  Second predicted as: 1 (0 times)\n",
      "\n",
      "Ground Truth Digit: 1\n",
      "  Most predicted as: 1 (45 times)\n",
      "  Second predicted as: 0 (0 times)\n",
      "\n",
      "Ground Truth Digit: 2\n",
      "  Most predicted as: 2 (20 times)\n",
      "  Second predicted as: 0 (0 times)\n",
      "\n",
      "Ground Truth Digit: 3\n",
      "  Most predicted as: 3 (19 times)\n",
      "  Second predicted as: 0 (0 times)\n",
      "\n",
      "Ground Truth Digit: 4\n",
      "  Most predicted as: 4 (41 times)\n",
      "  Second predicted as: 0 (0 times)\n",
      "\n",
      "Ground Truth Digit: 5\n",
      "  Most predicted as: 5 (10 times)\n",
      "  Second predicted as: 0 (0 times)\n",
      "\n",
      "Ground Truth Digit: 6\n",
      "  Most predicted as: 6 (10 times)\n",
      "  Second predicted as: 0 (3 times)\n",
      "\n",
      "Ground Truth Digit: 7\n",
      "  Most predicted as: 7 (10 times)\n",
      "  Second predicted as: 0 (0 times)\n",
      "\n",
      "Ground Truth Digit: 8\n",
      "  Most predicted as: 8 (9 times)\n",
      "  Second predicted as: 0 (0 times)\n",
      "\n",
      "Ground Truth Digit: 9\n",
      "  Most predicted as: 9 (45 times)\n",
      "  Second predicted as: 0 (0 times)\n",
      "\n",
      "---------------------------------\n",
      "Text Character Confusion:\n",
      "Character Confusion Matrix (rows=GT, cols=Prediction). Shape: (26, 26)\n",
      "\n",
      "Ground Truth Character: 'a' (count in GT = 170)\n",
      "  Most commonly predicted as: 'a' (count=161)\n",
      "  Second most predicted (possible confusion): 'e' (count=4)\n",
      "\n",
      "Ground Truth Character: 'b' (count in GT = 36)\n",
      "  Most commonly predicted as: 'b' (count=35)\n",
      "  Second most predicted (possible confusion): 'c' (count=1)\n",
      "\n",
      "Ground Truth Character: 'c' (count in GT = 44)\n",
      "  Most commonly predicted as: 'c' (count=44)\n",
      "  Second most predicted (possible confusion): 'a' (count=0)\n",
      "\n",
      "Ground Truth Character: 'd' (count in GT = 31)\n",
      "  Most commonly predicted as: 'd' (count=31)\n",
      "  Second most predicted (possible confusion): 'a' (count=0)\n",
      "\n",
      "Ground Truth Character: 'e' (count in GT = 92)\n",
      "  Most commonly predicted as: 'e' (count=92)\n",
      "  Second most predicted (possible confusion): 'a' (count=0)\n",
      "\n",
      "Ground Truth Character: 'f' (count in GT = 2)\n",
      "  Most commonly predicted as: 'f' (count=2)\n",
      "  Second most predicted (possible confusion): 'a' (count=0)\n",
      "\n",
      "Ground Truth Character: 'g' (count in GT = 15)\n",
      "  Most commonly predicted as: 'g' (count=15)\n",
      "  Second most predicted (possible confusion): 'a' (count=0)\n",
      "\n",
      "Ground Truth Character: 'h' (count in GT = 30)\n",
      "  Most commonly predicted as: 'h' (count=30)\n",
      "  Second most predicted (possible confusion): 'a' (count=0)\n",
      "\n",
      "Ground Truth Character: 'i' (count in GT = 67)\n",
      "  Most commonly predicted as: 'i' (count=66)\n",
      "  Second most predicted (possible confusion): 'a' (count=1)\n",
      "\n",
      "Ground Truth Character: 'j' (count in GT = 7)\n",
      "  Most commonly predicted as: 'j' (count=7)\n",
      "  Second most predicted (possible confusion): 'a' (count=0)\n",
      "\n",
      "Ground Truth Character: 'k' (count in GT = 8)\n",
      "  Most commonly predicted as: 'k' (count=7)\n",
      "  Second most predicted (possible confusion): 'r' (count=1)\n",
      "\n",
      "Ground Truth Character: 'l' (count in GT = 50)\n",
      "  Most commonly predicted as: 'l' (count=47)\n",
      "  Second most predicted (possible confusion): 'a' (count=1)\n",
      "\n",
      "Ground Truth Character: 'm' (count in GT = 46)\n",
      "  Most commonly predicted as: 'm' (count=46)\n",
      "  Second most predicted (possible confusion): 'a' (count=0)\n",
      "\n",
      "Ground Truth Character: 'n' (count in GT = 75)\n",
      "  Most commonly predicted as: 'n' (count=59)\n",
      "  Second most predicted (possible confusion): 'p' (count=4)\n",
      "\n",
      "Ground Truth Character: 'o' (count in GT = 63)\n",
      "  Most commonly predicted as: 'o' (count=63)\n",
      "  Second most predicted (possible confusion): 'a' (count=0)\n",
      "\n",
      "Ground Truth Character: 'p' (count in GT = 24)\n",
      "  Most commonly predicted as: 'p' (count=24)\n",
      "  Second most predicted (possible confusion): 'a' (count=0)\n",
      "\n",
      "Ground Truth Character: 'r' (count in GT = 64)\n",
      "  Most commonly predicted as: 'r' (count=64)\n",
      "  Second most predicted (possible confusion): 'a' (count=0)\n",
      "\n",
      "Ground Truth Character: 's' (count in GT = 46)\n",
      "  Most commonly predicted as: 's' (count=46)\n",
      "  Second most predicted (possible confusion): 'a' (count=0)\n",
      "\n",
      "Ground Truth Character: 't' (count in GT = 49)\n",
      "  Most commonly predicted as: 't' (count=49)\n",
      "  Second most predicted (possible confusion): 'a' (count=0)\n",
      "\n",
      "Ground Truth Character: 'u' (count in GT = 28)\n",
      "  Most commonly predicted as: 'u' (count=28)\n",
      "  Second most predicted (possible confusion): 'a' (count=0)\n",
      "\n",
      "Ground Truth Character: 'v' (count in GT = 1)\n",
      "  Most commonly predicted as: 'v' (count=1)\n",
      "  Second most predicted (possible confusion): 'a' (count=0)\n",
      "\n",
      "Ground Truth Character: 'w' (count in GT = 8)\n",
      "  Most commonly predicted as: 'w' (count=6)\n",
      "  Second most predicted (possible confusion): 'c' (count=1)\n",
      "\n",
      "Ground Truth Character: 'y' (count in GT = 3)\n",
      "  Most commonly predicted as: 'y' (count=3)\n",
      "  Second most predicted (possible confusion): 'a' (count=0)\n",
      "\n",
      "Ground Truth Character: 'z' (count in GT = 8)\n",
      "  Most commonly predicted as: 'z' (count=8)\n",
      "  Second most predicted (possible confusion): 'a' (count=0)\n",
      "\n",
      "---------------------------------\n",
      "Column: STATE\n",
      "  Exact Match Rate: 1.00\n",
      "Column: RACE_TYPE\n",
      "  Exact Match Rate: 1.00\n",
      "Column: CANDIDATE_NAME\n",
      "  Exact Match Rate: 0.66\n",
      "Column: CANDIDATE_PARTY\n",
      "  Exact Match Rate: 1.00\n",
      "===============================================\n",
      "\n",
      "\n",
      "=== 1980 Page 3 ===\n",
      "Overall Digit-Level Accuracy: 0.9876237623762376\n",
      "Average Levenshtein Distance (text): 0.0\n",
      "---------------------------------\n",
      "Digit-Level Accuracy (via confusion approach): 0.7772277227722773\n",
      "Digit Confusion Matrix (rows=ground-truth, cols=prediction):\n",
      "[[56  0  0  0  0  0  0  0  0  1]\n",
      " [ 0 62  0  0  0  0  0  0  0  0]\n",
      " [ 0  0 24  0  0  0  0  0  0  0]\n",
      " [ 0  1  0 14  0  0  0  0  0  0]\n",
      " [ 0  0  0  0 18  0  0  0  0  0]\n",
      " [ 0  0  0  0  0 15  0  0  0  0]\n",
      " [ 0  0  0  0  0  0 13  0  0  1]\n",
      " [ 0  0  0  0  0  0  0 14  0  0]\n",
      " [ 0  0  0  0  1  1  0  0 51  0]\n",
      " [ 0  0  0  0  0  0  0  0  0 47]]\n",
      "\n",
      "Ground Truth Digit: 0\n",
      "  Most predicted as: 0 (56 times)\n",
      "  Second predicted as: 9 (1 times)\n",
      "\n",
      "Ground Truth Digit: 1\n",
      "  Most predicted as: 1 (62 times)\n",
      "  Second predicted as: 0 (0 times)\n",
      "\n",
      "Ground Truth Digit: 2\n",
      "  Most predicted as: 2 (24 times)\n",
      "  Second predicted as: 0 (0 times)\n",
      "\n",
      "Ground Truth Digit: 3\n",
      "  Most predicted as: 3 (14 times)\n",
      "  Second predicted as: 1 (1 times)\n",
      "\n",
      "Ground Truth Digit: 4\n",
      "  Most predicted as: 4 (18 times)\n",
      "  Second predicted as: 0 (0 times)\n",
      "\n",
      "Ground Truth Digit: 5\n",
      "  Most predicted as: 5 (15 times)\n",
      "  Second predicted as: 0 (0 times)\n",
      "\n",
      "Ground Truth Digit: 6\n",
      "  Most predicted as: 6 (13 times)\n",
      "  Second predicted as: 9 (1 times)\n",
      "\n",
      "Ground Truth Digit: 7\n",
      "  Most predicted as: 7 (14 times)\n",
      "  Second predicted as: 0 (0 times)\n",
      "\n",
      "Ground Truth Digit: 8\n",
      "  Most predicted as: 8 (51 times)\n",
      "  Second predicted as: 4 (1 times)\n",
      "\n",
      "Ground Truth Digit: 9\n",
      "  Most predicted as: 9 (47 times)\n",
      "  Second predicted as: 0 (0 times)\n",
      "\n",
      "---------------------------------\n",
      "Text Character Confusion:\n",
      "Character Confusion Matrix (rows=GT, cols=Prediction). Shape: (26, 26)\n",
      "\n",
      "Ground Truth Character: 'a' (count in GT = 165)\n",
      "  Most commonly predicted as: 'a' (count=165)\n",
      "  Second most predicted (possible confusion): 'b' (count=0)\n",
      "\n",
      "Ground Truth Character: 'b' (count in GT = 23)\n",
      "  Most commonly predicted as: 'b' (count=23)\n",
      "  Second most predicted (possible confusion): 'a' (count=0)\n",
      "\n",
      "Ground Truth Character: 'c' (count in GT = 35)\n",
      "  Most commonly predicted as: 'c' (count=35)\n",
      "  Second most predicted (possible confusion): 'a' (count=0)\n",
      "\n",
      "Ground Truth Character: 'd' (count in GT = 33)\n",
      "  Most commonly predicted as: 'd' (count=33)\n",
      "  Second most predicted (possible confusion): 'a' (count=0)\n",
      "\n",
      "Ground Truth Character: 'e' (count in GT = 120)\n",
      "  Most commonly predicted as: 'e' (count=120)\n",
      "  Second most predicted (possible confusion): 'a' (count=0)\n",
      "\n",
      "Ground Truth Character: 'f' (count in GT = 9)\n",
      "  Most commonly predicted as: 'f' (count=9)\n",
      "  Second most predicted (possible confusion): 'a' (count=0)\n",
      "\n",
      "Ground Truth Character: 'g' (count in GT = 4)\n",
      "  Most commonly predicted as: 'g' (count=4)\n",
      "  Second most predicted (possible confusion): 'a' (count=0)\n",
      "\n",
      "Ground Truth Character: 'h' (count in GT = 23)\n",
      "  Most commonly predicted as: 'h' (count=23)\n",
      "  Second most predicted (possible confusion): 'a' (count=0)\n",
      "\n",
      "Ground Truth Character: 'i' (count in GT = 110)\n",
      "  Most commonly predicted as: 'i' (count=110)\n",
      "  Second most predicted (possible confusion): 'a' (count=0)\n",
      "\n",
      "Ground Truth Character: 'j' (count in GT = 4)\n",
      "  Most commonly predicted as: 'j' (count=4)\n",
      "  Second most predicted (possible confusion): 'a' (count=0)\n",
      "\n",
      "Ground Truth Character: 'k' (count in GT = 22)\n",
      "  Most commonly predicted as: 'k' (count=22)\n",
      "  Second most predicted (possible confusion): 'a' (count=0)\n",
      "\n",
      "Ground Truth Character: 'l' (count in GT = 59)\n",
      "  Most commonly predicted as: 'l' (count=59)\n",
      "  Second most predicted (possible confusion): 'a' (count=0)\n",
      "\n",
      "Ground Truth Character: 'm' (count in GT = 14)\n",
      "  Most commonly predicted as: 'm' (count=14)\n",
      "  Second most predicted (possible confusion): 'a' (count=0)\n",
      "\n",
      "Ground Truth Character: 'n' (count in GT = 118)\n",
      "  Most commonly predicted as: 'n' (count=118)\n",
      "  Second most predicted (possible confusion): 'a' (count=0)\n",
      "\n",
      "Ground Truth Character: 'o' (count in GT = 77)\n",
      "  Most commonly predicted as: 'o' (count=77)\n",
      "  Second most predicted (possible confusion): 'a' (count=0)\n",
      "\n",
      "Ground Truth Character: 'p' (count in GT = 30)\n",
      "  Most commonly predicted as: 'p' (count=30)\n",
      "  Second most predicted (possible confusion): 'a' (count=0)\n",
      "\n",
      "Ground Truth Character: 'r' (count in GT = 106)\n",
      "  Most commonly predicted as: 'r' (count=106)\n",
      "  Second most predicted (possible confusion): 'a' (count=0)\n",
      "\n",
      "Ground Truth Character: 's' (count in GT = 72)\n",
      "  Most commonly predicted as: 's' (count=72)\n",
      "  Second most predicted (possible confusion): 'a' (count=0)\n",
      "\n",
      "Ground Truth Character: 't' (count in GT = 57)\n",
      "  Most commonly predicted as: 't' (count=57)\n",
      "  Second most predicted (possible confusion): 'a' (count=0)\n",
      "\n",
      "Ground Truth Character: 'u' (count in GT = 29)\n",
      "  Most commonly predicted as: 'u' (count=29)\n",
      "  Second most predicted (possible confusion): 'a' (count=0)\n",
      "\n",
      "Ground Truth Character: 'v' (count in GT = 3)\n",
      "  Most commonly predicted as: 'v' (count=3)\n",
      "  Second most predicted (possible confusion): 'a' (count=0)\n",
      "\n",
      "Ground Truth Character: 'w' (count in GT = 12)\n",
      "  Most commonly predicted as: 'w' (count=12)\n",
      "  Second most predicted (possible confusion): 'a' (count=0)\n",
      "\n",
      "Ground Truth Character: 'y' (count in GT = 3)\n",
      "  Most commonly predicted as: 'y' (count=3)\n",
      "  Second most predicted (possible confusion): 'a' (count=0)\n",
      "\n",
      "Ground Truth Character: 'z' (count in GT = 28)\n",
      "  Most commonly predicted as: 'z' (count=28)\n",
      "  Second most predicted (possible confusion): 'a' (count=0)\n",
      "\n",
      "---------------------------------\n",
      "Column: STATE\n",
      "  Exact Match Rate: 1.00\n",
      "Column: RACE_TYPE\n",
      "  Exact Match Rate: 1.00\n",
      "Column: CANDIDATE_NAME\n",
      "  Exact Match Rate: 1.00\n",
      "Column: CANDIDATE_PARTY\n",
      "  Exact Match Rate: 1.00\n",
      "===============================================\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "############################\n",
    "# Combined Usage Example   #\n",
    "# with Numeric & Text Conf #\n",
    "############################\n",
    "\n",
    "import string\n",
    "\n",
    "# 1) Specify your column types\n",
    "numeric_columns = [\"YEAR\", \"CONGRESSIONAL_DISTRICT\", \"VOTES\"]\n",
    "text_columns    = [\"STATE\", \"RACE_TYPE\", \"CANDIDATE_NAME\", \"CANDIDATE_PARTY\"]\n",
    "\n",
    "# For text confusion, define a character list\n",
    "char_list = string.ascii_lowercase  # or expand to handle punctuation, uppercase, etc.\n",
    "\n",
    "########################################\n",
    "# 1920, page 9\n",
    "########################################\n",
    "df_human_1920_p9 = pd.read_csv(r\"1920pg9/1920pg9_correct.csv\")\n",
    "df_ai_1920_p9    = pd.read_csv(r\"1920pg9/1920p9_rawoutput.csv\")\n",
    "\n",
    "print(\"=== 1920 Page 9 ===\")\n",
    "\n",
    "# -- 1. Overall accuracy (digit-level + Levenshtein)\n",
    "results_1920_p9 = evaluate_accuracy(df_human_1920_p9, df_ai_1920_p9, numeric_columns, text_columns)\n",
    "print(\"Overall Digit-Level Accuracy:\", results_1920_p9[\"digit_level_accuracy\"])\n",
    "print(\"Average Levenshtein Distance (text):\", results_1920_p9[\"avg_levenshtein_dist\"])\n",
    "print(\"---------------------------------\")\n",
    "\n",
    "# -- 2. Numeric confusion\n",
    "num_results_1920_p9 = evaluate_numeric_with_confusion(df_human_1920_p9, df_ai_1920_p9, numeric_columns)\n",
    "print(\"Digit-Level Accuracy (via confusion approach):\", num_results_1920_p9[\"digit_level_accuracy\"])\n",
    "analyze_digit_confusion(num_results_1920_p9[\"confusion_matrix\"])\n",
    "print(\"---------------------------------\")\n",
    "\n",
    "# -- 3. Text confusion\n",
    "txt_results_1920_p9 = evaluate_text_with_char_confusion(df_human_1920_p9, df_ai_1920_p9, text_columns, char_list)\n",
    "master_char_conf_1920_p9 = txt_results_1920_p9[\"master_char_confusion\"]\n",
    "print(\"Text Character Confusion:\")\n",
    "analyze_char_confusion(master_char_conf_1920_p9, char_list)\n",
    "print(\"---------------------------------\")\n",
    "\n",
    "# Optionally, show per-column text exact match rates:\n",
    "for col, info in txt_results_1920_p9[\"per_column\"].items():\n",
    "    print(f\"Column: {col}\")\n",
    "    print(f\"  Exact Match Rate: {info['exact_match_rate']:.2f}\")\n",
    "print(\"===============================================\\n\\n\")\n",
    "\n",
    "\n",
    "########################################\n",
    "# 1940, page 1\n",
    "########################################\n",
    "df_human_1940_p1 = pd.read_csv(r\"1940pg1/1940pg1_correct.csv\")\n",
    "df_ai_1940_p1    = pd.read_csv(r\"1940pg1/1940pg1_rawoutput.csv\")\n",
    "\n",
    "print(\"=== 1940 Page 1 ===\")\n",
    "\n",
    "# -- 1. Overall accuracy\n",
    "results_1940_p1 = evaluate_accuracy(df_human_1940_p1, df_ai_1940_p1, numeric_columns, text_columns)\n",
    "print(\"Overall Digit-Level Accuracy:\", results_1940_p1[\"digit_level_accuracy\"])\n",
    "print(\"Average Levenshtein Distance (text):\", results_1940_p1[\"avg_levenshtein_dist\"])\n",
    "print(\"---------------------------------\")\n",
    "\n",
    "# -- 2. Numeric confusion\n",
    "num_results_1940_p1 = evaluate_numeric_with_confusion(df_human_1940_p1, df_ai_1940_p1, numeric_columns)\n",
    "print(\"Digit-Level Accuracy (via confusion approach):\", num_results_1940_p1[\"digit_level_accuracy\"])\n",
    "analyze_digit_confusion(num_results_1940_p1[\"confusion_matrix\"])\n",
    "print(\"---------------------------------\")\n",
    "\n",
    "# -- 3. Text confusion\n",
    "txt_results_1940_p1 = evaluate_text_with_char_confusion(df_human_1940_p1, df_ai_1940_p1, text_columns, char_list)\n",
    "master_char_conf_1940_p1 = txt_results_1940_p1[\"master_char_confusion\"]\n",
    "print(\"Text Character Confusion:\")\n",
    "analyze_char_confusion(master_char_conf_1940_p1, char_list)\n",
    "print(\"---------------------------------\")\n",
    "\n",
    "# Optionally, show per-column text exact match rates:\n",
    "for col, info in txt_results_1940_p1[\"per_column\"].items():\n",
    "    print(f\"Column: {col}\")\n",
    "    print(f\"  Exact Match Rate: {info['exact_match_rate']:.2f}\")\n",
    "print(\"===============================================\\n\\n\")\n",
    "\n",
    "\n",
    "########################################\n",
    "# 1980, page 3\n",
    "########################################\n",
    "df_human_1980_p3 = pd.read_csv(r\"1980pg3/1980pg3_correct.csv\")\n",
    "df_ai_1980_p3    = pd.read_csv(r\"1980pg3/1980pg3_rawoutput.csv\")\n",
    "\n",
    "print(\"=== 1980 Page 3 ===\")\n",
    "\n",
    "# -- 1. Overall accuracy\n",
    "results_1980_p3 = evaluate_accuracy(df_human_1980_p3, df_ai_1980_p3, numeric_columns, text_columns)\n",
    "print(\"Overall Digit-Level Accuracy:\", results_1980_p3[\"digit_level_accuracy\"])\n",
    "print(\"Average Levenshtein Distance (text):\", results_1980_p3[\"avg_levenshtein_dist\"])\n",
    "print(\"---------------------------------\")\n",
    "\n",
    "# -- 2. Numeric confusion\n",
    "num_results_1980_p3 = evaluate_numeric_with_confusion(df_human_1980_p3, df_ai_1980_p3, numeric_columns)\n",
    "print(\"Digit-Level Accuracy (via confusion approach):\", num_results_1980_p3[\"digit_level_accuracy\"])\n",
    "analyze_digit_confusion(num_results_1980_p3[\"confusion_matrix\"])\n",
    "print(\"---------------------------------\")\n",
    "\n",
    "# -- 3. Text confusion\n",
    "txt_results_1980_p3 = evaluate_text_with_char_confusion(df_human_1980_p3, df_ai_1980_p3, text_columns, char_list)\n",
    "master_char_conf_1980_p3 = txt_results_1980_p3[\"master_char_confusion\"]\n",
    "print(\"Text Character Confusion:\")\n",
    "analyze_char_confusion(master_char_conf_1980_p3, char_list)\n",
    "print(\"---------------------------------\")\n",
    "\n",
    "# Optionally, show per-column text exact match rates:\n",
    "for col, info in txt_results_1980_p3[\"per_column\"].items():\n",
    "    print(f\"Column: {col}\")\n",
    "    print(f\"  Exact Match Rate: {info['exact_match_rate']:.2f}\")\n",
    "print(\"===============================================\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 1940 NEW DATA ===\n",
      "Overall Digit-Level Accuracy: 0.9814772467413675\n",
      "Average Levenshtein Distance (text): 0.07880434782608696\n",
      "---------------------------------\n",
      "Digit-Level Accuracy (via confusion approach): 0.8705694031557283\n",
      "Digit Confusion Matrix (rows=ground-truth, cols=prediction):\n",
      "[[471   0   0   2   0   0   1   0   0   9]\n",
      " [  0 667   2   0   0   1   0   0   0   1]\n",
      " [  0   2 621   0   0   0   0   1   0   0]\n",
      " [  0   0   7 236   1   3   0   1   2   1]\n",
      " [  0   0   1   1 578   0   0   0   0   0]\n",
      " [  0   0   0   2   0 214   0   1   0   0]\n",
      " [  4   0   0   1   2   1 183   0   0   4]\n",
      " [  0   0   0   0   0   0   0 168   0   1]\n",
      " [  0   0   0   3   0   1   0   0 161   1]\n",
      " [  2   0   0   2   1   1   5   1   0 508]]\n",
      "\n",
      "Ground Truth Digit: 0\n",
      "  Most predicted as: 0 (471 times)\n",
      "  Second predicted as: 9 (9 times)\n",
      "\n",
      "Ground Truth Digit: 1\n",
      "  Most predicted as: 1 (667 times)\n",
      "  Second predicted as: 2 (2 times)\n",
      "\n",
      "Ground Truth Digit: 2\n",
      "  Most predicted as: 2 (621 times)\n",
      "  Second predicted as: 1 (2 times)\n",
      "\n",
      "Ground Truth Digit: 3\n",
      "  Most predicted as: 3 (236 times)\n",
      "  Second predicted as: 2 (7 times)\n",
      "\n",
      "Ground Truth Digit: 4\n",
      "  Most predicted as: 4 (578 times)\n",
      "  Second predicted as: 2 (1 times)\n",
      "\n",
      "Ground Truth Digit: 5\n",
      "  Most predicted as: 5 (214 times)\n",
      "  Second predicted as: 3 (2 times)\n",
      "\n",
      "Ground Truth Digit: 6\n",
      "  Most predicted as: 6 (183 times)\n",
      "  Second predicted as: 0 (4 times)\n",
      "\n",
      "Ground Truth Digit: 7\n",
      "  Most predicted as: 7 (168 times)\n",
      "  Second predicted as: 9 (1 times)\n",
      "\n",
      "Ground Truth Digit: 8\n",
      "  Most predicted as: 8 (161 times)\n",
      "  Second predicted as: 3 (3 times)\n",
      "\n",
      "Ground Truth Digit: 9\n",
      "  Most predicted as: 9 (508 times)\n",
      "  Second predicted as: 6 (5 times)\n",
      "\n",
      "---------------------------------\n",
      "Text Character Confusion:\n",
      "Character Confusion Matrix (rows=GT, cols=Prediction). Shape: (26, 26)\n",
      "\n",
      "Ground Truth Character: 'a' (count in GT = 1340)\n",
      "  Most commonly predicted as: 'a' (count=1335)\n",
      "  Second most predicted (possible confusion): 'r' (count=2)\n",
      "\n",
      "Ground Truth Character: 'b' (count in GT = 276)\n",
      "  Most commonly predicted as: 'b' (count=274)\n",
      "  Second most predicted (possible confusion): 'p' (count=2)\n",
      "\n",
      "Ground Truth Character: 'c' (count in GT = 732)\n",
      "  Most commonly predicted as: 'c' (count=725)\n",
      "  Second most predicted (possible confusion): 'n' (count=2)\n",
      "\n",
      "Ground Truth Character: 'd' (count in GT = 430)\n",
      "  Most commonly predicted as: 'd' (count=429)\n",
      "  Second most predicted (possible confusion): 's' (count=1)\n",
      "\n",
      "Ground Truth Character: 'e' (count in GT = 1726)\n",
      "  Most commonly predicted as: 'e' (count=1711)\n",
      "  Second most predicted (possible confusion): 'n' (count=3)\n",
      "\n",
      "Ground Truth Character: 'f' (count in GT = 128)\n",
      "  Most commonly predicted as: 'f' (count=126)\n",
      "  Second most predicted (possible confusion): 'b' (count=1)\n",
      "\n",
      "Ground Truth Character: 'g' (count in GT = 191)\n",
      "  Most commonly predicted as: 'g' (count=188)\n",
      "  Second most predicted (possible confusion): 'c' (count=1)\n",
      "\n",
      "Ground Truth Character: 'h' (count in GT = 543)\n",
      "  Most commonly predicted as: 'h' (count=543)\n",
      "  Second most predicted (possible confusion): 'a' (count=0)\n",
      "\n",
      "Ground Truth Character: 'i' (count in GT = 1032)\n",
      "  Most commonly predicted as: 'i' (count=1027)\n",
      "  Second most predicted (possible confusion): 'a' (count=1)\n",
      "\n",
      "Ground Truth Character: 'j' (count in GT = 114)\n",
      "  Most commonly predicted as: 'j' (count=110)\n",
      "  Second most predicted (possible confusion): 'e' (count=1)\n",
      "\n",
      "Ground Truth Character: 'k' (count in GT = 138)\n",
      "  Most commonly predicted as: 'k' (count=134)\n",
      "  Second most predicted (possible confusion): 'e' (count=2)\n",
      "\n",
      "Ground Truth Character: 'l' (count in GT = 681)\n",
      "  Most commonly predicted as: 'l' (count=675)\n",
      "  Second most predicted (possible confusion): 'a' (count=2)\n",
      "\n",
      "Ground Truth Character: 'm' (count in GT = 512)\n",
      "  Most commonly predicted as: 'm' (count=511)\n",
      "  Second most predicted (possible confusion): 'n' (count=1)\n",
      "\n",
      "Ground Truth Character: 'n' (count in GT = 955)\n",
      "  Most commonly predicted as: 'n' (count=946)\n",
      "  Second most predicted (possible confusion): 'o' (count=2)\n",
      "\n",
      "Ground Truth Character: 'o' (count in GT = 1011)\n",
      "  Most commonly predicted as: 'o' (count=1006)\n",
      "  Second most predicted (possible confusion): 'a' (count=1)\n",
      "\n",
      "Ground Truth Character: 'p' (count in GT = 365)\n",
      "  Most commonly predicted as: 'p' (count=363)\n",
      "  Second most predicted (possible confusion): 'n' (count=1)\n",
      "\n",
      "Ground Truth Character: 'r' (count in GT = 1155)\n",
      "  Most commonly predicted as: 'r' (count=1146)\n",
      "  Second most predicted (possible confusion): 'n' (count=3)\n",
      "\n",
      "Ground Truth Character: 's' (count in GT = 973)\n",
      "  Most commonly predicted as: 's' (count=971)\n",
      "  Second most predicted (possible confusion): 'e' (count=1)\n",
      "\n",
      "Ground Truth Character: 't' (count in GT = 829)\n",
      "  Most commonly predicted as: 't' (count=825)\n",
      "  Second most predicted (possible confusion): 'c' (count=1)\n",
      "\n",
      "Ground Truth Character: 'u' (count in GT = 533)\n",
      "  Most commonly predicted as: 'u' (count=533)\n",
      "  Second most predicted (possible confusion): 'a' (count=0)\n",
      "\n",
      "Ground Truth Character: 'v' (count in GT = 132)\n",
      "  Most commonly predicted as: 'v' (count=132)\n",
      "  Second most predicted (possible confusion): 'a' (count=0)\n",
      "\n",
      "Ground Truth Character: 'w' (count in GT = 147)\n",
      "  Most commonly predicted as: 'w' (count=145)\n",
      "  Second most predicted (possible confusion): 'm' (count=1)\n",
      "\n",
      "Ground Truth Character: 'x' (count in GT = 10)\n",
      "  Most commonly predicted as: 'x' (count=9)\n",
      "  Second most predicted (possible confusion): 'y' (count=1)\n",
      "\n",
      "Ground Truth Character: 'y' (count in GT = 98)\n",
      "  Most commonly predicted as: 'y' (count=98)\n",
      "  Second most predicted (possible confusion): 'a' (count=0)\n",
      "\n",
      "Ground Truth Character: 'z' (count in GT = 14)\n",
      "  Most commonly predicted as: 'z' (count=14)\n",
      "  Second most predicted (possible confusion): 'a' (count=0)\n",
      "\n",
      "---------------------------------\n",
      "[DEBUG] Row 233, col 'YEAR' -> true_num=1942.0, pred_num=nan, skipping (NaN/inf).\n",
      "[DEBUG] Row 0, col 'CONGRESSIONAL_DISTRICT' -> true_num=nan, pred_num=nan, skipping (NaN/inf).\n",
      "[DEBUG] Row 1, col 'CONGRESSIONAL_DISTRICT' -> true_num=nan, pred_num=nan, skipping (NaN/inf).\n",
      "[DEBUG] Row 19, col 'CONGRESSIONAL_DISTRICT' -> true_num=nan, pred_num=nan, skipping (NaN/inf).\n",
      "[DEBUG] Row 45, col 'CONGRESSIONAL_DISTRICT' -> true_num=nan, pred_num=nan, skipping (NaN/inf).\n",
      "[DEBUG] Row 46, col 'CONGRESSIONAL_DISTRICT' -> true_num=nan, pred_num=nan, skipping (NaN/inf).\n",
      "[DEBUG] Row 47, col 'CONGRESSIONAL_DISTRICT' -> true_num=nan, pred_num=nan, skipping (NaN/inf).\n",
      "[DEBUG] Row 48, col 'CONGRESSIONAL_DISTRICT' -> true_num=nan, pred_num=nan, skipping (NaN/inf).\n",
      "[DEBUG] Row 49, col 'CONGRESSIONAL_DISTRICT' -> true_num=nan, pred_num=nan, skipping (NaN/inf).\n",
      "[DEBUG] Row 50, col 'CONGRESSIONAL_DISTRICT' -> true_num=nan, pred_num=nan, skipping (NaN/inf).\n",
      "[DEBUG] Row 51, col 'CONGRESSIONAL_DISTRICT' -> true_num=nan, pred_num=nan, skipping (NaN/inf).\n",
      "[DEBUG] Row 52, col 'CONGRESSIONAL_DISTRICT' -> true_num=nan, pred_num=nan, skipping (NaN/inf).\n",
      "[DEBUG] Row 53, col 'CONGRESSIONAL_DISTRICT' -> true_num=nan, pred_num=nan, skipping (NaN/inf).\n",
      "[DEBUG] Row 54, col 'CONGRESSIONAL_DISTRICT' -> true_num=nan, pred_num=nan, skipping (NaN/inf).\n",
      "[DEBUG] Row 55, col 'CONGRESSIONAL_DISTRICT' -> true_num=nan, pred_num=nan, skipping (NaN/inf).\n",
      "[DEBUG] Row 56, col 'CONGRESSIONAL_DISTRICT' -> true_num=nan, pred_num=nan, skipping (NaN/inf).\n",
      "[DEBUG] Row 57, col 'CONGRESSIONAL_DISTRICT' -> true_num=nan, pred_num=nan, skipping (NaN/inf).\n",
      "[DEBUG] Row 58, col 'CONGRESSIONAL_DISTRICT' -> true_num=nan, pred_num=nan, skipping (NaN/inf).\n",
      "[DEBUG] Row 59, col 'CONGRESSIONAL_DISTRICT' -> true_num=nan, pred_num=nan, skipping (NaN/inf).\n",
      "[DEBUG] Row 60, col 'CONGRESSIONAL_DISTRICT' -> true_num=nan, pred_num=nan, skipping (NaN/inf).\n",
      "[DEBUG] Row 61, col 'CONGRESSIONAL_DISTRICT' -> true_num=nan, pred_num=nan, skipping (NaN/inf).\n",
      "[DEBUG] Row 78, col 'CONGRESSIONAL_DISTRICT' -> true_num=nan, pred_num=nan, skipping (NaN/inf).\n",
      "[DEBUG] Row 79, col 'CONGRESSIONAL_DISTRICT' -> true_num=nan, pred_num=nan, skipping (NaN/inf).\n",
      "[DEBUG] Row 80, col 'CONGRESSIONAL_DISTRICT' -> true_num=nan, pred_num=nan, skipping (NaN/inf).\n",
      "[DEBUG] Row 103, col 'CONGRESSIONAL_DISTRICT' -> true_num=nan, pred_num=nan, skipping (NaN/inf).\n",
      "[DEBUG] Row 104, col 'CONGRESSIONAL_DISTRICT' -> true_num=nan, pred_num=nan, skipping (NaN/inf).\n",
      "[DEBUG] Row 105, col 'CONGRESSIONAL_DISTRICT' -> true_num=nan, pred_num=nan, skipping (NaN/inf).\n",
      "[DEBUG] Row 106, col 'CONGRESSIONAL_DISTRICT' -> true_num=nan, pred_num=nan, skipping (NaN/inf).\n",
      "[DEBUG] Row 122, col 'CONGRESSIONAL_DISTRICT' -> true_num=nan, pred_num=nan, skipping (NaN/inf).\n",
      "[DEBUG] Row 123, col 'CONGRESSIONAL_DISTRICT' -> true_num=nan, pred_num=nan, skipping (NaN/inf).\n",
      "[DEBUG] Row 141, col 'CONGRESSIONAL_DISTRICT' -> true_num=nan, pred_num=nan, skipping (NaN/inf).\n",
      "[DEBUG] Row 142, col 'CONGRESSIONAL_DISTRICT' -> true_num=nan, pred_num=nan, skipping (NaN/inf).\n",
      "[DEBUG] Row 143, col 'CONGRESSIONAL_DISTRICT' -> true_num=nan, pred_num=nan, skipping (NaN/inf).\n",
      "[DEBUG] Row 144, col 'CONGRESSIONAL_DISTRICT' -> true_num=nan, pred_num=nan, skipping (NaN/inf).\n",
      "[DEBUG] Row 145, col 'CONGRESSIONAL_DISTRICT' -> true_num=nan, pred_num=nan, skipping (NaN/inf).\n",
      "[DEBUG] Row 173, col 'CONGRESSIONAL_DISTRICT' -> true_num=nan, pred_num=nan, skipping (NaN/inf).\n",
      "[DEBUG] Row 174, col 'CONGRESSIONAL_DISTRICT' -> true_num=nan, pred_num=nan, skipping (NaN/inf).\n",
      "[DEBUG] Row 175, col 'CONGRESSIONAL_DISTRICT' -> true_num=nan, pred_num=nan, skipping (NaN/inf).\n",
      "[DEBUG] Row 176, col 'CONGRESSIONAL_DISTRICT' -> true_num=nan, pred_num=nan, skipping (NaN/inf).\n",
      "[DEBUG] Row 177, col 'CONGRESSIONAL_DISTRICT' -> true_num=nan, pred_num=nan, skipping (NaN/inf).\n",
      "[DEBUG] Row 178, col 'CONGRESSIONAL_DISTRICT' -> true_num=nan, pred_num=nan, skipping (NaN/inf).\n",
      "[DEBUG] Row 179, col 'CONGRESSIONAL_DISTRICT' -> true_num=nan, pred_num=nan, skipping (NaN/inf).\n",
      "[DEBUG] Row 184, col 'CONGRESSIONAL_DISTRICT' -> true_num=nan, pred_num=nan, skipping (NaN/inf).\n",
      "[DEBUG] Row 185, col 'CONGRESSIONAL_DISTRICT' -> true_num=nan, pred_num=nan, skipping (NaN/inf).\n",
      "[DEBUG] Row 186, col 'CONGRESSIONAL_DISTRICT' -> true_num=nan, pred_num=nan, skipping (NaN/inf).\n",
      "[DEBUG] Row 233, col 'CONGRESSIONAL_DISTRICT' -> true_num=20.0, pred_num=nan, skipping (NaN/inf).\n",
      "[DEBUG] Row 240, col 'CONGRESSIONAL_DISTRICT' -> true_num=nan, pred_num=nan, skipping (NaN/inf).\n",
      "[DEBUG] Row 241, col 'CONGRESSIONAL_DISTRICT' -> true_num=nan, pred_num=nan, skipping (NaN/inf).\n",
      "[DEBUG] Row 242, col 'CONGRESSIONAL_DISTRICT' -> true_num=nan, pred_num=nan, skipping (NaN/inf).\n",
      "[DEBUG] Row 243, col 'CONGRESSIONAL_DISTRICT' -> true_num=nan, pred_num=nan, skipping (NaN/inf).\n",
      "[DEBUG] Row 250, col 'CONGRESSIONAL_DISTRICT' -> true_num=nan, pred_num=nan, skipping (NaN/inf).\n",
      "[DEBUG] Row 251, col 'CONGRESSIONAL_DISTRICT' -> true_num=nan, pred_num=nan, skipping (NaN/inf).\n",
      "[DEBUG] Row 252, col 'CONGRESSIONAL_DISTRICT' -> true_num=nan, pred_num=nan, skipping (NaN/inf).\n",
      "[DEBUG] Row 253, col 'CONGRESSIONAL_DISTRICT' -> true_num=nan, pred_num=nan, skipping (NaN/inf).\n",
      "[DEBUG] Row 264, col 'CONGRESSIONAL_DISTRICT' -> true_num=nan, pred_num=nan, skipping (NaN/inf).\n",
      "[DEBUG] Row 300, col 'CONGRESSIONAL_DISTRICT' -> true_num=nan, pred_num=nan, skipping (NaN/inf).\n",
      "[DEBUG] Row 301, col 'CONGRESSIONAL_DISTRICT' -> true_num=nan, pred_num=nan, skipping (NaN/inf).\n",
      "[DEBUG] Row 302, col 'CONGRESSIONAL_DISTRICT' -> true_num=nan, pred_num=nan, skipping (NaN/inf).\n",
      "[DEBUG] Row 303, col 'CONGRESSIONAL_DISTRICT' -> true_num=nan, pred_num=nan, skipping (NaN/inf).\n",
      "[DEBUG] Row 304, col 'CONGRESSIONAL_DISTRICT' -> true_num=nan, pred_num=nan, skipping (NaN/inf).\n",
      "[DEBUG] Row 305, col 'CONGRESSIONAL_DISTRICT' -> true_num=nan, pred_num=nan, skipping (NaN/inf).\n",
      "[DEBUG] Row 233, col 'VOTES' -> true_num=792.0, pred_num=nan, skipping (NaN/inf).\n",
      "\n",
      "[DEBUG] Parsing & Skipping Summary for Numeric Columns:\n",
      "  YEAR: parsed=367, parse_fail=0, nan_or_inf_skips=1\n",
      "  CONGRESSIONAL_DISTRICT: parsed=308, parse_fail=0, nan_or_inf_skips=60\n",
      "  VOTES: parsed=367, parse_fail=0, nan_or_inf_skips=1\n",
      "\n",
      "Overall MAE: 108.50095969289828\n",
      "Overall MAPE: 0.007084184319516782\n",
      "Column: YEAR, MAE=0.0, MAPE=0.0\n",
      "Column: CONGRESSIONAL_DISTRICT, MAE=0.003246753246753247, MAPE=0.0016233766233766235\n",
      "Column: VOTES, MAE=308.0572207084469, MAPE=0.01875128081999043\n",
      "Column: STATE\n",
      "  Exact Match Rate: 1.00\n",
      "Column: RACE_TYPE\n",
      "  Exact Match Rate: 1.00\n",
      "Column: CANDIDATE_NAME\n",
      "  Exact Match Rate: 0.94\n",
      "Column: CANDIDATE_PARTY\n",
      "  Exact Match Rate: 0.99\n",
      "===============================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# New Usage Example for the 1940 CSVs\n",
    "\n",
    "import pandas as pd\n",
    "import string\n",
    "\n",
    "# 1) Specify columns\n",
    "numeric_columns = [\"YEAR\", \"CONGRESSIONAL_DISTRICT\", \"VOTES\"]\n",
    "text_columns    = [\"STATE\", \"RACE_TYPE\", \"CANDIDATE_NAME\", \"CANDIDATE_PARTY\"]\n",
    "\n",
    "# 2) Load the new CSV files\n",
    "df_human_1940new = pd.read_csv(r\"1940full/1940human.csv\")\n",
    "df_ai_1940new    = pd.read_csv(r\"1940full/1940raw.csv\")\n",
    "\n",
    "print(\"=== 1940 NEW DATA ===\")\n",
    "\n",
    "# -- A) Overall accuracy (digit-level + Levenshtein)\n",
    "results_1940new = evaluate_accuracy(\n",
    "    df_human_1940new,\n",
    "    df_ai_1940new,\n",
    "    numeric_columns,\n",
    "    text_columns\n",
    ")\n",
    "print(\"Overall Digit-Level Accuracy:\", results_1940new[\"digit_level_accuracy\"])\n",
    "print(\"Average Levenshtein Distance (text):\", results_1940new[\"avg_levenshtein_dist\"])\n",
    "print(\"---------------------------------\")\n",
    "\n",
    "# -- B) Numeric confusion\n",
    "num_results_1940new = evaluate_numeric_with_confusion(\n",
    "    df_human_1940new,\n",
    "    df_ai_1940new,\n",
    "    numeric_columns\n",
    ")\n",
    "print(\"Digit-Level Accuracy (via confusion approach):\", num_results_1940new[\"digit_level_accuracy\"])\n",
    "analyze_digit_confusion(num_results_1940new[\"confusion_matrix\"])\n",
    "print(\"---------------------------------\")\n",
    "\n",
    "# -- C) Text character confusion\n",
    "char_list = string.ascii_lowercase  # or expand for punctuation, uppercase, etc.\n",
    "txt_results_1940new = evaluate_text_with_char_confusion(\n",
    "    df_human_1940new,\n",
    "    df_ai_1940new,\n",
    "    text_columns,\n",
    "    char_list\n",
    ")\n",
    "master_char_conf_1940new = txt_results_1940new[\"master_char_confusion\"]\n",
    "print(\"Text Character Confusion:\")\n",
    "analyze_char_confusion(master_char_conf_1940new, char_list)\n",
    "print(\"---------------------------------\")\n",
    "\n",
    "# -- D) Absolute and Percentage Errors\n",
    "err_results = evaluate_numeric_errors(df_human_1940new, df_ai_1940new, numeric_columns, debug=True)\n",
    "\n",
    "print(\"Overall MAE:\", err_results[\"overall_mae\"])\n",
    "print(\"Overall MAPE:\", err_results[\"overall_mape\"])\n",
    "for col, stats in err_results[\"per_column\"].items():\n",
    "    print(f\"Column: {col}, MAE={stats['mae']}, MAPE={stats['mape']}\")\n",
    "\n",
    "# -- Optionally, show per-column text exact match rates:\n",
    "for col, info in txt_results_1940new[\"per_column\"].items():\n",
    "    print(f\"Column: {col}\")\n",
    "    print(f\"  Exact Match Rate: {info['exact_match_rate']:.2f}\")\n",
    "print(\"===============================================\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 1920 Page 9 ===\n",
      "Digit-Level Accuracy (overall): 0.9850427350427351\n",
      "Avg Levenshtein Distance (text): 0.03353658536585366\n",
      "---------------------------------\n",
      "Digit-Level Accuracy (via confusion approach): 0.8675213675213675\n",
      "Confusion Matrix (rows=ground-truth, cols=prediction):\n",
      " [[178   0   0   0   0   0   0   0   0   1]\n",
      " [  0 132   0   0   0   0   0   0   0   0]\n",
      " [  0   0 132   0   0   0   0   0   0   0]\n",
      " [  0   0   0  44   2   0   0   0   0   0]\n",
      " [  0   0   1   0  44   0   0   0   0   0]\n",
      " [  0   0   0   1   0  44   0   0   2   1]\n",
      " [  0   1   0   0   0   0  36   0   1   0]\n",
      " [  0   0   0   0   0   0   0  46   0   0]\n",
      " [  0   0   0   1   0   0   0   0  36   0]\n",
      " [  2   0   0   0   1   0   0   0   0 120]] \n",
      "\n",
      "Digit Confusion Matrix (rows=ground-truth, cols=prediction):\n",
      "[[178   0   0   0   0   0   0   0   0   1]\n",
      " [  0 132   0   0   0   0   0   0   0   0]\n",
      " [  0   0 132   0   0   0   0   0   0   0]\n",
      " [  0   0   0  44   2   0   0   0   0   0]\n",
      " [  0   0   1   0  44   0   0   0   0   0]\n",
      " [  0   0   0   1   0  44   0   0   2   1]\n",
      " [  0   1   0   0   0   0  36   0   1   0]\n",
      " [  0   0   0   0   0   0   0  46   0   0]\n",
      " [  0   0   0   1   0   0   0   0  36   0]\n",
      " [  2   0   0   0   1   0   0   0   0 120]]\n",
      "\n",
      "Ground Truth Digit: 0\n",
      "  Most predicted as: 0 (178 times)\n",
      "  Second predicted as: 9 (1 times)\n",
      "\n",
      "Ground Truth Digit: 1\n",
      "  Most predicted as: 1 (132 times)\n",
      "  Second predicted as: 0 (0 times)\n",
      "\n",
      "Ground Truth Digit: 2\n",
      "  Most predicted as: 2 (132 times)\n",
      "  Second predicted as: 0 (0 times)\n",
      "\n",
      "Ground Truth Digit: 3\n",
      "  Most predicted as: 3 (44 times)\n",
      "  Second predicted as: 4 (2 times)\n",
      "\n",
      "Ground Truth Digit: 4\n",
      "  Most predicted as: 4 (44 times)\n",
      "  Second predicted as: 2 (1 times)\n",
      "\n",
      "Ground Truth Digit: 5\n",
      "  Most predicted as: 5 (44 times)\n",
      "  Second predicted as: 8 (2 times)\n",
      "\n",
      "Ground Truth Digit: 6\n",
      "  Most predicted as: 6 (36 times)\n",
      "  Second predicted as: 1 (1 times)\n",
      "\n",
      "Ground Truth Digit: 7\n",
      "  Most predicted as: 7 (46 times)\n",
      "  Second predicted as: 0 (0 times)\n",
      "\n",
      "Ground Truth Digit: 8\n",
      "  Most predicted as: 8 (36 times)\n",
      "  Second predicted as: 3 (1 times)\n",
      "\n",
      "Ground Truth Digit: 9\n",
      "  Most predicted as: 9 (120 times)\n",
      "  Second predicted as: 0 (2 times)\n",
      "\n",
      "===============================================\n",
      "\n",
      "\n",
      "=== 1940 Page 1 ===\n",
      "Digit-Level Accuracy (overall): 0.9908814589665653\n",
      "Avg Levenshtein Distance (text): 0.7241379310344828\n",
      "---------------------------------\n",
      "Digit-Level Accuracy (via confusion approach): 0.8115501519756839\n",
      "Confusion Matrix (rows=ground-truth, cols=prediction):\n",
      " [[58  0  0  0  0  0  0  0  0  0]\n",
      " [ 0 45  0  0  0  0  0  0  0  0]\n",
      " [ 0  0 20  0  0  0  0  0  0  0]\n",
      " [ 0  0  0 19  0  0  0  0  0  0]\n",
      " [ 0  0  0  0 41  0  0  0  0  0]\n",
      " [ 0  0  0  0  0 10  0  0  0  0]\n",
      " [ 3  0  0  0  0  0 10  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 10  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  9  0]\n",
      " [ 0  0  0  0  0  0  0  0  0 45]] \n",
      "\n",
      "Digit Confusion Matrix (rows=ground-truth, cols=prediction):\n",
      "[[58  0  0  0  0  0  0  0  0  0]\n",
      " [ 0 45  0  0  0  0  0  0  0  0]\n",
      " [ 0  0 20  0  0  0  0  0  0  0]\n",
      " [ 0  0  0 19  0  0  0  0  0  0]\n",
      " [ 0  0  0  0 41  0  0  0  0  0]\n",
      " [ 0  0  0  0  0 10  0  0  0  0]\n",
      " [ 3  0  0  0  0  0 10  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 10  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  9  0]\n",
      " [ 0  0  0  0  0  0  0  0  0 45]]\n",
      "\n",
      "Ground Truth Digit: 0\n",
      "  Most predicted as: 0 (58 times)\n",
      "  Second predicted as: 1 (0 times)\n",
      "\n",
      "Ground Truth Digit: 1\n",
      "  Most predicted as: 1 (45 times)\n",
      "  Second predicted as: 0 (0 times)\n",
      "\n",
      "Ground Truth Digit: 2\n",
      "  Most predicted as: 2 (20 times)\n",
      "  Second predicted as: 0 (0 times)\n",
      "\n",
      "Ground Truth Digit: 3\n",
      "  Most predicted as: 3 (19 times)\n",
      "  Second predicted as: 0 (0 times)\n",
      "\n",
      "Ground Truth Digit: 4\n",
      "  Most predicted as: 4 (41 times)\n",
      "  Second predicted as: 0 (0 times)\n",
      "\n",
      "Ground Truth Digit: 5\n",
      "  Most predicted as: 5 (10 times)\n",
      "  Second predicted as: 0 (0 times)\n",
      "\n",
      "Ground Truth Digit: 6\n",
      "  Most predicted as: 6 (10 times)\n",
      "  Second predicted as: 0 (3 times)\n",
      "\n",
      "Ground Truth Digit: 7\n",
      "  Most predicted as: 7 (10 times)\n",
      "  Second predicted as: 0 (0 times)\n",
      "\n",
      "Ground Truth Digit: 8\n",
      "  Most predicted as: 8 (9 times)\n",
      "  Second predicted as: 0 (0 times)\n",
      "\n",
      "Ground Truth Digit: 9\n",
      "  Most predicted as: 9 (45 times)\n",
      "  Second predicted as: 0 (0 times)\n",
      "\n",
      "===============================================\n",
      "\n",
      "\n",
      "=== 1980 Page 3 ===\n",
      "Digit-Level Accuracy (overall): 0.9876237623762376\n",
      "Avg Levenshtein Distance (text): 0.0\n",
      "---------------------------------\n",
      "Digit-Level Accuracy (via confusion approach): 0.7772277227722773\n",
      "Confusion Matrix (rows=ground-truth, cols=prediction):\n",
      " [[56  0  0  0  0  0  0  0  0  1]\n",
      " [ 0 62  0  0  0  0  0  0  0  0]\n",
      " [ 0  0 24  0  0  0  0  0  0  0]\n",
      " [ 0  1  0 14  0  0  0  0  0  0]\n",
      " [ 0  0  0  0 18  0  0  0  0  0]\n",
      " [ 0  0  0  0  0 15  0  0  0  0]\n",
      " [ 0  0  0  0  0  0 13  0  0  1]\n",
      " [ 0  0  0  0  0  0  0 14  0  0]\n",
      " [ 0  0  0  0  1  1  0  0 51  0]\n",
      " [ 0  0  0  0  0  0  0  0  0 47]] \n",
      "\n",
      "Digit Confusion Matrix (rows=ground-truth, cols=prediction):\n",
      "[[56  0  0  0  0  0  0  0  0  1]\n",
      " [ 0 62  0  0  0  0  0  0  0  0]\n",
      " [ 0  0 24  0  0  0  0  0  0  0]\n",
      " [ 0  1  0 14  0  0  0  0  0  0]\n",
      " [ 0  0  0  0 18  0  0  0  0  0]\n",
      " [ 0  0  0  0  0 15  0  0  0  0]\n",
      " [ 0  0  0  0  0  0 13  0  0  1]\n",
      " [ 0  0  0  0  0  0  0 14  0  0]\n",
      " [ 0  0  0  0  1  1  0  0 51  0]\n",
      " [ 0  0  0  0  0  0  0  0  0 47]]\n",
      "\n",
      "Ground Truth Digit: 0\n",
      "  Most predicted as: 0 (56 times)\n",
      "  Second predicted as: 9 (1 times)\n",
      "\n",
      "Ground Truth Digit: 1\n",
      "  Most predicted as: 1 (62 times)\n",
      "  Second predicted as: 0 (0 times)\n",
      "\n",
      "Ground Truth Digit: 2\n",
      "  Most predicted as: 2 (24 times)\n",
      "  Second predicted as: 0 (0 times)\n",
      "\n",
      "Ground Truth Digit: 3\n",
      "  Most predicted as: 3 (14 times)\n",
      "  Second predicted as: 1 (1 times)\n",
      "\n",
      "Ground Truth Digit: 4\n",
      "  Most predicted as: 4 (18 times)\n",
      "  Second predicted as: 0 (0 times)\n",
      "\n",
      "Ground Truth Digit: 5\n",
      "  Most predicted as: 5 (15 times)\n",
      "  Second predicted as: 0 (0 times)\n",
      "\n",
      "Ground Truth Digit: 6\n",
      "  Most predicted as: 6 (13 times)\n",
      "  Second predicted as: 9 (1 times)\n",
      "\n",
      "Ground Truth Digit: 7\n",
      "  Most predicted as: 7 (14 times)\n",
      "  Second predicted as: 0 (0 times)\n",
      "\n",
      "Ground Truth Digit: 8\n",
      "  Most predicted as: 8 (51 times)\n",
      "  Second predicted as: 4 (1 times)\n",
      "\n",
      "Ground Truth Digit: 9\n",
      "  Most predicted as: 9 (47 times)\n",
      "  Second predicted as: 0 (0 times)\n",
      "\n",
      "===============================================\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Combined Usage Example with Confusion Analysis\n",
    "\n",
    "# 1) Specify column types\n",
    "numeric_columns = [\"YEAR\", \"CONGRESSIONAL_DISTRICT\", \"VOTES\"]\n",
    "text_columns = [\"STATE\", \"RACE_TYPE\", \"CANDIDATE_NAME\", \"CANDIDATE_PARTY\"]\n",
    "\n",
    "########################################\n",
    "# 1920, page 9\n",
    "########################################\n",
    "df_human_1920_p9 = pd.read_csv(r\"1920pg9/1920pg9_correct.csv\")\n",
    "df_ai_1920_p9    = pd.read_csv(r\"1920pg9/1920p9_rawoutput.csv\")\n",
    "\n",
    "# -- Overall accuracy (digit-level + Levenshtein) --\n",
    "results_1920_p9 = evaluate_accuracy(df_human_1920_p9, df_ai_1920_p9, numeric_columns, text_columns)\n",
    "print(\"=== 1920 Page 9 ===\")\n",
    "print(\"Digit-Level Accuracy (overall):\", results_1920_p9[\"digit_level_accuracy\"])\n",
    "print(\"Avg Levenshtein Distance (text):\", results_1920_p9[\"avg_levenshtein_dist\"])\n",
    "print(\"---------------------------------\")\n",
    "\n",
    "# -- Confusion matrix for numeric columns --\n",
    "conf_results_1920_p9 = evaluate_numeric_with_confusion(df_human_1920_p9, df_ai_1920_p9, numeric_columns)\n",
    "digit_acc_1920_p9 = conf_results_1920_p9[\"digit_level_accuracy\"]\n",
    "conf_mat_1920_p9  = conf_results_1920_p9[\"confusion_matrix\"]\n",
    "\n",
    "print(\"Digit-Level Accuracy (via confusion approach):\", digit_acc_1920_p9)\n",
    "print(\"Confusion Matrix (rows=ground-truth, cols=prediction):\\n\", conf_mat_1920_p9, \"\\n\")\n",
    "analyze_digit_confusion(conf_mat_1920_p9)\n",
    "print(\"===============================================\\n\\n\")\n",
    "\n",
    "\n",
    "########################################\n",
    "# 1940, page 1\n",
    "########################################\n",
    "df_human_1940_p1 = pd.read_csv(r\"1940pg1/1940pg1_correct.csv\")\n",
    "df_ai_1940_p1    = pd.read_csv(r\"1940pg1/1940pg1_rawoutput.csv\")\n",
    "\n",
    "# -- Overall accuracy --\n",
    "results_1940_p1 = evaluate_accuracy(df_human_1940_p1, df_ai_1940_p1, numeric_columns, text_columns)\n",
    "print(\"=== 1940 Page 1 ===\")\n",
    "print(\"Digit-Level Accuracy (overall):\", results_1940_p1[\"digit_level_accuracy\"])\n",
    "print(\"Avg Levenshtein Distance (text):\", results_1940_p1[\"avg_levenshtein_dist\"])\n",
    "print(\"---------------------------------\")\n",
    "\n",
    "# -- Confusion matrix for numeric columns --\n",
    "conf_results_1940_p1 = evaluate_numeric_with_confusion(df_human_1940_p1, df_ai_1940_p1, numeric_columns)\n",
    "digit_acc_1940_p1 = conf_results_1940_p1[\"digit_level_accuracy\"]\n",
    "conf_mat_1940_p1  = conf_results_1940_p1[\"confusion_matrix\"]\n",
    "\n",
    "print(\"Digit-Level Accuracy (via confusion approach):\", digit_acc_1940_p1)\n",
    "print(\"Confusion Matrix (rows=ground-truth, cols=prediction):\\n\", conf_mat_1940_p1, \"\\n\")\n",
    "analyze_digit_confusion(conf_mat_1940_p1)\n",
    "print(\"===============================================\\n\\n\")\n",
    "\n",
    "\n",
    "########################################\n",
    "# 1980, page 3\n",
    "########################################\n",
    "df_human_1980_p3 = pd.read_csv(r\"1980pg3/1980pg3_correct.csv\")\n",
    "df_ai_1980_p3    = pd.read_csv(r\"1980pg3/1980pg3_rawoutput.csv\")\n",
    "\n",
    "# -- Overall accuracy --\n",
    "results_1980_p3 = evaluate_accuracy(df_human_1980_p3, df_ai_1980_p3, numeric_columns, text_columns)\n",
    "print(\"=== 1980 Page 3 ===\")\n",
    "print(\"Digit-Level Accuracy (overall):\", results_1980_p3[\"digit_level_accuracy\"])\n",
    "print(\"Avg Levenshtein Distance (text):\", results_1980_p3[\"avg_levenshtein_dist\"])\n",
    "print(\"---------------------------------\")\n",
    "\n",
    "# -- Confusion matrix for numeric columns --\n",
    "conf_results_1980_p3 = evaluate_numeric_with_confusion(df_human_1980_p3, df_ai_1980_p3, numeric_columns)\n",
    "digit_acc_1980_p3 = conf_results_1980_p3[\"digit_level_accuracy\"]\n",
    "conf_mat_1980_p3  = conf_results_1980_p3[\"confusion_matrix\"]\n",
    "\n",
    "print(\"Digit-Level Accuracy (via confusion approach):\", digit_acc_1980_p3)\n",
    "print(\"Confusion Matrix (rows=ground-truth, cols=prediction):\\n\", conf_mat_1980_p3, \"\\n\")\n",
    "analyze_digit_confusion(conf_mat_1980_p3)\n",
    "print(\"===============================================\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 1920 Page 9 ===\n",
      "Digit-Level Accuracy: 0.9850427350427351\n",
      "Avg Levenshtein Distance: 0.03353658536585366\n",
      "---------------------------------\n",
      "\n",
      "=== 1940 Page 1 ===\n",
      "Digit-Level Accuracy: 0.9908814589665653\n",
      "Avg Levenshtein Distance: 0.7241379310344828\n",
      "---------------------------------\n",
      "\n",
      "=== 1940 Page 1 ===\n",
      "Digit-Level Accuracy: 0.9876237623762376\n",
      "Avg Levenshtein Distance: 0.0\n",
      "---------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "numeric_columns = [\"YEAR\", \"CONGRESSIONAL_DISTRICT\", \"VOTES\"]\n",
    "text_columns = [\"STATE\", \"RACE_TYPE\", \"CANDIDATE_NAME\", \"CANDIDATE_PARTY\"]\n",
    "\n",
    "# 1920, page 9\n",
    "df_human_1920_p9 = pd.read_csv(r\"1920pg9/1920pg9_correct.csv\")\n",
    "df_ai_1920_p9    = pd.read_csv(r\"1920pg9/1920p9_rawoutput.csv\")\n",
    "\n",
    "results_1920_p1 = evaluate_accuracy(df_human_1920_p9, df_ai_1920_p9, numeric_columns, text_columns)\n",
    "print(\"=== 1920 Page 9 ===\")\n",
    "print(\"Digit-Level Accuracy:\", results_1920_p1[\"digit_level_accuracy\"])\n",
    "print(\"Avg Levenshtein Distance:\", results_1920_p1[\"avg_levenshtein_dist\"])\n",
    "print(\"---------------------------------\\n\")\n",
    "\n",
    "# 1940, page 1\n",
    "df_human_1940_p1 = pd.read_csv(r\"1940pg1/1940pg1_correct.csv\")\n",
    "df_ai_1940_p1    = pd.read_csv(r\"1940pg1/1940pg1_rawoutput.csv\")\n",
    "\n",
    "results_1940_p1 = evaluate_accuracy(df_human_1940_p1, df_ai_1940_p1, numeric_columns, text_columns)\n",
    "print(\"=== 1940 Page 1 ===\")\n",
    "print(\"Digit-Level Accuracy:\", results_1940_p1[\"digit_level_accuracy\"])\n",
    "print(\"Avg Levenshtein Distance:\", results_1940_p1[\"avg_levenshtein_dist\"])\n",
    "print(\"---------------------------------\\n\")\n",
    "\n",
    "# 4) 1980, page 3\n",
    "df_human_1980_p3 = pd.read_csv(r\"1980pg3/1980pg3_correct.csv\")\n",
    "df_ai_1980_p3    = pd.read_csv(r\"1980pg3/1980pg3_rawoutput.csv\")\n",
    "\n",
    "results_1940_p1 = evaluate_accuracy(df_human_1980_p3, df_ai_1980_p3, numeric_columns, text_columns)\n",
    "print(\"=== 1940 Page 1 ===\")\n",
    "print(\"Digit-Level Accuracy:\", results_1980_p3[\"digit_level_accuracy\"])\n",
    "print(\"Avg Levenshtein Distance:\", results_1980_p3[\"avg_levenshtein_dist\"])\n",
    "print(\"---------------------------------\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
